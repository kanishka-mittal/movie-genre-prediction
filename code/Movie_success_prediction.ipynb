{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFPwj0ySkaMg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# !unzip '/content/gdrive/My Drive/Movies/posters.zip'\n",
        "!unzip '/content/gdrive/My Drive/Movies/Earning Prediction/Dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FwFflPQgkaq9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "import copy\n",
        "import shutil\n",
        "from urllib.request import urlopen\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IcAjvWypkdOJ"
      },
      "outputs": [],
      "source": [
        "dfwithearnings = pd.read_csv(\"./gdrive/My Drive/Movies/movies_metadata.csv\",encoding='ISO-8859-1')\n",
        "dfwithoutearnings = pd.read_csv(\"./gdrive/My Drive/Movies/MovieGenre.csv\",encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLAjKP0ikeyx",
        "outputId": "b59096b1-016f-4122-8f2e-76f424f732e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4849, 7)\n"
          ]
        }
      ],
      "source": [
        "dfwithearnings = dfwithearnings[dfwithearnings.poster_path.notnull()]\n",
        "dfwithearnings = dfwithearnings[dfwithearnings.budget != 0]\n",
        "dfwithearnings = dfwithearnings[dfwithearnings.revenue != 0]\n",
        "dfwithearnings = dfwithearnings[dfwithearnings.genres.notnull()]\n",
        "dfwithearnings = dfwithearnings[dfwithearnings.original_title.notnull()]\n",
        "dfwithearnings = dfwithearnings[dfwithearnings.title.notnull()]\n",
        "dfwithearnings = dfwithearnings[dfwithearnings.release_date.notnull()]\n",
        "\n",
        "dfwithearnings['year'] = dfwithearnings['release_date'].str[:4]\n",
        "\n",
        "\n",
        "dfwithearnings['Title'] = dfwithearnings['title'] + \" (\" + dfwithearnings['year'] + \")\"\n",
        "\n",
        "dffinal = pd.merge(dfwithearnings, dfwithoutearnings, on=\"Title\")\n",
        "dffinal = dffinal[['Title', 'budget', 'revenue', 'Genre', 'Poster']]\n",
        "dffinal['budget'] = pd.to_numeric(dffinal['budget'])\n",
        "dffinal = dffinal.loc[dffinal['budget']!= 0]\n",
        "dffinal = pd.DataFrame.reset_index(dffinal)\n",
        "dffinal = dffinal.drop(['index'], axis = 1)\n",
        "\n",
        "dffinal['revtobud'] = dffinal['revenue']/dffinal['budget']\n",
        "dffinal['pe_range'] = np.where(dffinal['revtobud']>=1, 'high-earnings', 'low-earnings')\n",
        "print(dffinal.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VStNakeSlAqp",
        "outputId": "1e574e3a-1360-44d4-a878-17c8003dc354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4849, 7)\n"
          ]
        }
      ],
      "source": [
        "directory = 'Dataset/samplemovieposters'\n",
        "directory2 = 'Dataset/samplemoviepostersprocessed'\n",
        "\n",
        "\n",
        "print(dffinal.shape)\n",
        "for i in range(dffinal.shape[0]):\n",
        "    filename = str(i) + \".jpg\"\n",
        "    address = os.path.join(directory, filename)\n",
        "    image = Image.open(address)\n",
        "    image = image.resize((224, 224), Image.BILINEAR)\n",
        "    if dffinal['pe_range'][i] == \"high-earnings\":\n",
        "        newaddress = directory2 + \"/high\"\n",
        "        if not os.path.isdir(newaddress):\n",
        "            os.makedirs(newaddress)\n",
        "        newaddress = os.path.join(newaddress, filename)\n",
        "        \n",
        "        image.save(newaddress)\n",
        "    else:\n",
        "        newaddress = directory2 + \"/low\"\n",
        "        if not os.path.isdir(newaddress):\n",
        "            os.makedirs(newaddress)\n",
        "        newaddress = os.path.join(newaddress, filename)\n",
        "        image.save(newaddress)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0Er8yZX4kg-y"
      },
      "outputs": [],
      "source": [
        "do_transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.RandomCrop((280,280), padding=None, pad_if_needed=True, fill=0, padding_mode='constant'),\n",
        "        torchvision.transforms.Resize((224), interpolation=2),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "])\n",
        "directory3 = 'Dataset/samplemoviepostersprocessed'\n",
        "dataset = torchvision.datasets.ImageFolder(directory3, transform=do_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PTPr7vsqk--Z"
      },
      "outputs": [],
      "source": [
        "validation_split = .2\n",
        "shuffle_dataset = True\n",
        "batch_size = 64\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "dataloaders_dict = {\n",
        "    'train': torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
        "                                         sampler=train_sampler),\n",
        "    'val': torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                       sampler=valid_sampler)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzTOXLFAlcDe",
        "outputId": "496fa678-ddf1-4f8b-be89-9dd3ff70f4d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "\n",
        "num_classes = len(dataset.classes)\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "i9Pd6ilMlncn"
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, learning_rate=0.01, early_stopping = 5, patience = 0.005):\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        params_to_update = self.get_parameters()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        self.early_stopping = early_stopping\n",
        "\n",
        "        self.patience = patience\n",
        "\n",
        "    def _loop(self, data_loader):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_data_count = 0\n",
        "\n",
        "        for X, Y in data_loader:\n",
        "            inputs = X.to(self.device)\n",
        "            labels = Y.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            running_corrects += torch.sum(self.computes_accuracy(outputs, labels.data))\n",
        "\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            total_data_count += len(X)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        epoch_loss = running_loss / total_data_count\n",
        "        epoch_acc = running_corrects.double() / total_data_count\n",
        "\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def computes_accuracy(self, outputs, targets):\n",
        "        _, preds = outputs.topk(1, 1, True, True)\n",
        "        preds = preds.t()\n",
        "        correct = preds.eq(targets.view(1, -1).expand_as(preds))\n",
        "        correct_k = correct[:1].view(-1).float()\n",
        "        return correct_k\n",
        "\n",
        "    def train(self, data_loader):\n",
        "        self.model.train()\n",
        "        return self._loop(data_loader)\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_data_count = 0\n",
        "        for X, Y in data_loader:\n",
        "            inputs = X.to(self.device)\n",
        "            labels = Y.to(self.device)\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            running_corrects += torch.sum(self.computes_accuracy(outputs, labels.data))\n",
        "\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            total_data_count += len(X)\n",
        "        \n",
        "        eval_loss = running_loss / total_data_count\n",
        "        eval_acc = running_corrects.double() / total_data_count\n",
        "\n",
        "        return eval_loss, eval_acc\n",
        "\n",
        "    def fit(self, dataloaders_dict, num_epochs=10):\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        early_stopping_counter = self.early_stopping\n",
        "        time_fit_start = time.time()\n",
        "        train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
        "\n",
        "        best_epoch_info = {\n",
        "            'model_wts':copy.deepcopy(self.model.state_dict()),\n",
        "            'loss':1e10\n",
        "        }\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            time_epoch_start = time.time()\n",
        "\n",
        "            train_loss, train_acc = self.train(dataloaders_dict['train'])\n",
        "            val_loss, val_acc = self.evaluate(dataloaders_dict['val'])\n",
        "            \n",
        "            train_losses.append(train_loss)\n",
        "            test_losses.append(val_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "            test_accuracies.append(val_acc)\n",
        "\n",
        "            current_learning_rate = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            print(\"Epoch {:2} in {:.0f}s || Train loss={:.3f}, acc={:.3f} | Val loss={:.3f}, acc={:.3f} | LR={}\".format(epoch+1, time.time() - time_epoch_start, train_loss, train_acc, val_loss, val_acc, current_learning_rate))\n",
        "\n",
        "            if val_loss < best_epoch_info['loss']:\n",
        "                best_epoch_info = {\n",
        "                    'model_wts':copy.deepcopy(self.model.state_dict()),\n",
        "                    'loss':val_loss,\n",
        "                    'epoch':epoch,\n",
        "                    'metrics':{\n",
        "                        'train_loss':train_loss,\n",
        "                        'val_loss':val_loss,\n",
        "                        'train_acc':train_acc,\n",
        "                        'val_acc':val_acc\n",
        "                    }\n",
        "                }\n",
        "\n",
        "            if len(test_losses) > 1:\n",
        "                if val_loss-test_losses[len(test_losses)-2] >= -1*self.patience:\n",
        "                    early_stopping_counter -= 1\n",
        "                else:\n",
        "                    early_stopping_counter = self.early_stopping\n",
        "\n",
        "            if early_stopping_counter == 0:\n",
        "                print(\"Early Stop\")\n",
        "                break\n",
        "\n",
        "        time_elapsed = time.time() - time_fit_start\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        self.model.load_state_dict(best_epoch_info['model_wts'])\n",
        "\n",
        "        print('Loaded best epoch : ', best_epoch_info['epoch'])\n",
        "        return train_losses, test_losses\n",
        "\n",
        "    def get_parameters(self):\n",
        "        print(\"Layers with params to learn:\")\n",
        "        params_to_update = []\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                params_to_update.append(param)\n",
        "\n",
        "        print('\\t', len(params_to_update), 'layers')\n",
        "        return params_to_update\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifl2SkU-li-n",
        "outputId": "59e9e119-83f7-45ed-bfa7-0b7146f9d541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers with params to learn:\n",
            "\t 62 layers\n",
            "Starting training...\n",
            "Epoch  1 in 21s || Train loss=0.604, acc=0.715 | Val loss=0.603, acc=0.710 | LR=0.01\n",
            "Epoch  2 in 21s || Train loss=0.601, acc=0.715 | Val loss=0.601, acc=0.711 | LR=0.01\n",
            "Epoch  3 in 21s || Train loss=0.603, acc=0.713 | Val loss=0.607, acc=0.711 | LR=0.01\n",
            "Epoch  4 in 24s || Train loss=0.602, acc=0.715 | Val loss=0.601, acc=0.711 | LR=0.01\n",
            "Epoch  5 in 21s || Train loss=0.599, acc=0.715 | Val loss=0.601, acc=0.711 | LR=0.01\n",
            "Epoch  6 in 21s || Train loss=0.599, acc=0.715 | Val loss=0.605, acc=0.711 | LR=0.01\n",
            "Epoch  7 in 21s || Train loss=0.600, acc=0.715 | Val loss=0.601, acc=0.711 | LR=0.01\n",
            "Epoch  8 in 21s || Train loss=0.598, acc=0.715 | Val loss=0.601, acc=0.711 | LR=0.01\n",
            "Epoch  9 in 22s || Train loss=0.598, acc=0.715 | Val loss=0.602, acc=0.711 | LR=0.01\n",
            "Early Stop\n",
            "Training complete in 3m 13s\n",
            "Loaded best epoch :  1\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1000)\n",
        "trainer = Trainer(model)\n",
        "\n",
        "trainloss, testloss = trainer.fit(dataloaders_dict, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Y9D1oPZBmPuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cf76ae-38b7-4f42-e532-955a96c104f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy : 0.711\n"
          ]
        }
      ],
      "source": [
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "total_data_count = 0\n",
        "\n",
        "def computes_accuracy(outputs, targets, k=1):\n",
        "        _, preds = outputs.topk(k, 1, True, True)\n",
        "        preds = preds.t()\n",
        "        correct = preds.eq(targets.view(1, -1).expand_as(preds))\n",
        "        correct_k = correct[:k].view(-1).float()\n",
        "        return correct_k\n",
        "\n",
        "for X, Y in dataloaders_dict['val']:\n",
        "            inputs = X.to(device)\n",
        "            labels = Y.to(device)\n",
        "            inputs, labels = inputs.cuda(), labels.cuda() \n",
        "\n",
        "            outputs = model(inputs)\n",
        "            acc = torch.sum(computes_accuracy(outputs, labels.data, 1))\n",
        "            running_corrects += acc\n",
        "            total_data_count += len(X)\n",
        "            \n",
        "ValidationAcc = running_corrects.double() / total_data_count\n",
        "print(\"Validation Accuracy : {:.3f}\".format(ValidationAcc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n"
      ],
      "metadata": {
        "id": "VFCGU2IGslDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae37bcf7-fbcb-4740-9e6d-b011b8e3eefb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(trainloss)\n",
        "plt.plot(testloss)\n",
        "plt.legend(['Training Loss','Validation Loss'])\n",
        "plt.xticks([2,4,6,8,10,12,14,16,18])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "krHcaDITyjXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count = 0\n",
        "# i = 0\n",
        "# for link in dffinal['Poster'].iteritems():\n",
        "#   if(i < 4840):\n",
        "#     i+=1\n",
        "#     continue\n",
        "#   if(link[1] != 'nan'):\n",
        "#     save_as = \"samplemovieposters/\" + str(i) + \".jpg\"\n",
        "#     url = link[1]\n",
        "#     try:\n",
        "#         file = urlopen(url)\n",
        "#         content = file.read()\n",
        "#         count+=1\n",
        "#         print(count)\n",
        "#     except:\n",
        "#         continue\n",
        "#     # Save to file\n",
        "#     with open(save_as, 'wb') as download:\n",
        "#         download.write(content)\n",
        "#     i+=1\n",
        "# print(count)"
      ],
      "metadata": {
        "id": "dIX2rIb22KGP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMvVHqFWN6S7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}